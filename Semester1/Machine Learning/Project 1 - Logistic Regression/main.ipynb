{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a987d4f",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1 - CSE 574 - INTRODUCTION TO MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367844e6",
   "metadata": {},
   "source": [
    "__PART 1 - Logistic Regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read csv file\n",
    "data=pd.read_csv(\"diabetes.csv\")\n",
    "data.head()\n",
    "x=data[data.columns[0:-1]]\n",
    "y=data[data.columns[-1]]\n",
    "\n",
    "#Splitting dataset into training, test and validation sets\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4,random_state=42)\n",
    "x_val,x_test,y_val,y_test=train_test_split(x_test,y_test,test_size=0.5,random_state=42)\n",
    "print(x_train.shape,x_test.shape,x_val.shape,y_train.shape,y_test.shape,y_val.shape)\n",
    "\n",
    "#retriving values and storing them \n",
    "x_train=x_train.values\n",
    "y_train=y_train.values\n",
    "x_test=x_test.values\n",
    "y_test=y_test.values\n",
    "x_val=x_val.values\n",
    "y_val=y_val.values\n",
    "\n",
    "\n",
    "#reshaping the training, test and validation arrays as required\n",
    "x_train=x_train.transpose()\n",
    "y_train=y_train.reshape(1,x_train.shape[1])\n",
    "x_test=x_test.transpose()\n",
    "y_test=y_test.reshape(1,x_test.shape[1])\n",
    "x_val=x_val.transpose()\n",
    "y_val=y_val.reshape(1,x_val.shape[1])\n",
    "\n",
    "#checking the shape of all input arrays\n",
    "print(x_train.shape,x_test.shape,x_val.shape,y_train.shape,y_test.shape,y_val.shape)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    \n",
    "\n",
    "def model(x, y, learning_rate, iterations):\n",
    "    m=x_train.shape[1]\n",
    "    n=x_train.shape[0]\n",
    "    \n",
    "    W=np.zeros((n,1))\n",
    "    B=0\n",
    "    cost_list = []\n",
    "    for i in range(iterations):\n",
    "        Z=np.dot(W.transpose(),x)+B\n",
    "        A=sigmoid(Z)        \n",
    "        \n",
    "        #Cost Function\n",
    "        cost=-(1/m)*np.sum(y*np.log(A)+(1-y)*np.log(1-A))\n",
    "        \n",
    "        #Gradient Descent\n",
    "        dW=(1/m)*np.dot((A-y),x.transpose())\n",
    "        dB=(1/m)*np.sum(A-y)\n",
    "        \n",
    "        W=W-learning_rate*dW.transpose()\n",
    "        B=B-learning_rate*dB\n",
    "        \n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if(i%(iterations/10)==0):\n",
    "            print(\"cost after \",i,\" iterations is: \",cost)\n",
    "            #learning_rate=learning_rate-learning_rate/55\n",
    "    return W,B,cost_list\n",
    "\n",
    "iterations=1000000\n",
    "learning_rate= 0.00023\n",
    "\n",
    "W,B,cost_list=model(x_train,y_train,learning_rate=learning_rate,iterations=iterations)\n",
    "\n",
    "plt.plot(np.arange(iterations), cost_list)\n",
    "plt.show()\n",
    "\n",
    "def accuracy(X, Y, W, B):\n",
    "    \n",
    "    Z = np.dot(W.T, X) + B\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    A = A > 0.5\n",
    "    \n",
    "    A = np.array(A, dtype = 'int64')\n",
    "    \n",
    "    acc = (1 - np.sum(np.absolute(A - Y))/Y.shape[1])*100\n",
    "    print(round(acc, 2), \"%\")\n",
    "\n",
    "print(\"The accuracy of training dataset is: \")\n",
    "accuracy(x_train, y_train, W, B)\n",
    "print(\"The accuracy of validation dataset is: \")\n",
    "accuracy(x_val, y_val, W, B)\n",
    "print(\"The accuracy of test dataset is: \")\n",
    "accuracy(x_test, y_test, W, B)\n",
    "print(W)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bb6dd",
   "metadata": {},
   "source": [
    "__PART 2 - Neural Networks__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data=pd.read_csv(\"diabetes.csv\")\n",
    "properties = list(data.columns.values)\n",
    "properties.remove('Outcome')\n",
    "X = data[properties]\n",
    "y = data['Outcome']\n",
    "\n",
    "#Splitting dataset into training, test and validation sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "X_val,X_test,y_val,y_test=train_test_split(X_test,y_test,test_size=0.5,random_state=42)\n",
    "print(X_train.shape,X_test.shape,X_val.shape,y_train.shape,y_test.shape,y_val.shape)\n",
    "    \n",
    "#retriving values and storing them \n",
    "X_train=X_train.values\n",
    "y_train=y_train.values\n",
    "X_test=X_test.values\n",
    "y_test=y_test.values\n",
    "X_val=X_val.values\n",
    "y_val=y_val.values\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(8,)),\n",
    "    keras.layers.Dense(12, kernel_regularizer=regularizers.L1(0.001), bias_regularizer=regularizers.L1(0.001), activation=tf.nn.relu),\n",
    "\tkeras.layers.Dense(1, kernel_regularizer=regularizers.L1(0.001), bias_regularizer=regularizers.L1(0.001), activation=tf.nn.sigmoid),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_val, y_val))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "train_loss, train_acc = model.evaluate(X_val, y_val)\n",
    "print(\"\\n Training set Accuracy:\", train_acc)\n",
    "print(\"\\n Training set loss:\", train_loss)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(\"\\n Validation Accuracy:\", val_acc)\n",
    "print(\"\\n Validation loss:\", val_loss)\n",
    "\n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,101)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc_train = history.history['accuracy']\n",
    "acc_val = history.history['val_accuracy']\n",
    "epochs = range(1,101)\n",
    "plt.plot(epochs, acc_train, 'g', label='Training Accuracy')\n",
    "plt.plot(epochs, acc_val, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efde20",
   "metadata": {},
   "source": [
    "__PART 3 - Implementing Dropout Regularization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b3589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
